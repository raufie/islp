{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8845030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just stuff that i find usefull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd90c2",
   "metadata": {},
   "source": [
    "# Prediction vs Inference\n",
    "\n",
    "Give a set of features Xn, with Xi being a random variable. We set out to predict Y.\n",
    "\n",
    "$ Y = f(X) + \\epsilon$\n",
    "\n",
    "The $\\epsilon$ is irreducible error, Y can be explained by a function of X but $\\epsilon$ stays there, it's because $\\epsilon$ is often due to unexplained variation in the data (which could be due to variables that are not part of the function $f$. So imagine you're prediction cancer rates based on age alone, your irreducible error will be huge because you've left out major features or random variables or \"regressors\" as they're called in regression. If you'd used test results, gender, height, weight, bmi etc. you could do a better job and youre $\\epsilon$ would be lower)\n",
    "\n",
    "$\\epsilon$ sets an upper bound on the accuracy of f basically, you can't do better than that bound unless you add the variables that explain away the unexplained variation.\n",
    "\n",
    "$E[Y-\\hat{Y}]^2 = E[f(X) + \\epsilon - \\hat{f}(x)]^2$\n",
    "\n",
    "$E[Y-\\hat{Y}]^2 = E[f(X) - \\hat{f}(x)]^2 + E[\\epsilon]^2$ \n",
    "\n",
    "Second term on the right is obviously the second moment (variance)\n",
    "\n",
    "Reducible error = $E[f(X) - \\hat{f}(x)]^2$\n",
    "\n",
    "Irreducible error = $Var[\\epsilon]$\n",
    "\n",
    "\n",
    "# Inference\n",
    "In prediction we don't care what $\\hat{f}$ is, we just care about the accuracy of our $\\hat{Y}$, but in inference we care about different associations between the Variables that lead to the output being $\\hat{Y}$. So while making good predictions, we'll also know what contributed to those predictions. So far cancer score example, was it the age or diet or exercise or VO2 max or what have you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c191312",
   "metadata": {},
   "source": [
    "# Bias variance tradeoff\n",
    "## Variance\n",
    "Variance of f_hat is the variance you'd get if you trained f_hat on a lot of different datasets to predict the same value. so f_hat(x0) where f_hat comes from a bunch of datasets. E[f_hat[xo]] is the expected value here, variance follows from that.\n",
    "\n",
    "See variance of an estimator as its flexibility, more variance -> more flexible, less variance -> less general. \n",
    "\n",
    "## Bias\n",
    "\n",
    "Bias is the error introduced by using a method f_hat for estimating f that is has less complexity than the true f. It's error accrued when you try to estimate a real world problem using a simple method (whereas the real world problem as a much more complicated method (the DGP is much more complicated than the estimator)).\n",
    "\n",
    "\n",
    "For example, fitting a linear model on a dataset wihch isn't linear (lets say x^2); it's obvious that you'll get high error here.. That error comes from bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185d26f",
   "metadata": {},
   "source": [
    "# Classification (bayes pr)\n",
    "\n",
    "\n",
    "Bayes classifier assigns each sample in the test set the accurate conditional probability (Pr(class | observation))\n",
    "\n",
    "Now it's possible that an observation may have multiple classes as it often is in the real world (its conditional probability).\n",
    "\n",
    "Using above we come up with bayes error rate = $1 - \\sum_{i}^{n}{(max_{j}(Pr(y_i=j|x_i)))}$\n",
    "\n",
    "Decision bounding is where P(class|obs) = 0.5\n",
    "\n",
    "It's a theoratical thing, much like the irreducible error in regression, you can't go lower than the bayes error rate\n",
    "\n",
    "### KNN\n",
    "\n",
    "It just assigns the test observation a class based on its closest neighbors using the euclidean distance (n. of neighbors is defined by K)\n",
    "\n",
    "given K, and a set of neighbors $N_o$; $P(class|obs) = \\frac{1}{k} \\sum_{i=1}^{N_o}{I(class(i)==class)} $\n",
    "\n",
    "Similar to how basic probability is calculated given events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5bb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1678464",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
